{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WARNING: This notebook only works in Google Colab. DO NOT attempt to install the required packages in your local environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5TfCE2aegm9"
      },
      "outputs": [],
      "source": [
        "# This notebook takes live video and samples frames to feed to a trained image recognition model and displays the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbWamyjofIg-"
      },
      "outputs": [],
      "source": [
        "# Import video libraries\n",
        "import os\n",
        "from IPython.display import display, Javascript, Image, clear_output\n",
        "# from google.colab.output import eval_js # Uncomment for Google Colab!\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the Spotipy Library if using in Google Colab\n",
        "# !pip install spotipy # Uncomment for Google Colab! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc42RIiEf1t8"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRKkSu00nUCr"
      },
      "outputs": [],
      "source": [
        "# load our models\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "emotion_model_html = keras.models.load_model('emotion_model.h5')\n",
        "# Define the emotion labels\n",
        "emotion_labels = [\"Angry\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXN8_VlEmLlK"
      },
      "outputs": [],
      "source": [
        "# Create code that takes an image from the video every 10 seconds, and saves the last 5 images\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  jpeg_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(jpeg_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  return bbox_bytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSW0-KMnwdCn"
      },
      "outputs": [],
      "source": [
        "# our snapshot instantiation code\n",
        "snapshot_dir = 'snapshots'\n",
        "os.makedirs(snapshot_dir, exist_ok=True)\n",
        "# Initialize a list to store the last 5 snapshots\n",
        "last_snapshots = []\n",
        "# Initialize a variable to store the last snapshot time\n",
        "last_snapshot_time = time.time()\n",
        "# Initialize the emotion list\n",
        "emotion_data = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "zHDWxxPyoZaI",
        "outputId": "7a7a6640-da79-4018-e89e-f5632c90b0c6"
      },
      "outputs": [],
      "source": [
        "''' This Block just takes images from the live video and stores them. We'll do\n",
        "inference in the block below'''\n",
        "\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "# Initialize a counter\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray)  # ensure face_cascade is defined\n",
        "    '''# get face bounding box for overlay\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)'''\n",
        "    # get face bounding box for overlay\n",
        "    for (x,y,w,h) in faces:\n",
        "        # Convert coordinates to integers before drawing the rectangle\n",
        "        x1 = int(.7 * x)\n",
        "        y1 = int(.7 * y)\n",
        "        x2 = int(x + (1.3 * w))\n",
        "        y2 = int(y + (1.3 * h))\n",
        "        bbox_array = cv2.rectangle(bbox_array,(x1, y1),(x2, y2),(255,0,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    # Check if 5 seconds have passed\n",
        "    current_time = time.time()\n",
        "    if current_time - last_snapshot_time >= 5:\n",
        "        count += 1\n",
        "        last_snapshot_time = current_time\n",
        "        snapshot_path = os.path.join(snapshot_dir, f'snapshot_{count}.jpg')\n",
        "        cv2.imwrite(snapshot_path, img)\n",
        "        last_snapshots.append(snapshot_path)\n",
        "        if len(last_snapshots) > 5:\n",
        "            os.remove(last_snapshots.pop(0))\n",
        "\n",
        "        print(f'Snapshot saved: {snapshot_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1XibPy6Mp14s",
        "outputId": "e8a6011f-9663-41a6-913e-6d2cda3bc810"
      },
      "outputs": [],
      "source": [
        "''' This Block takes images from the live video and runs inference on them with the html model file'''\n",
        "# Start streaming video from webcam\n",
        "video_stream()\n",
        "# Label for video\n",
        "label_html = 'Capturing...'\n",
        "# Initialize bounding box to empty\n",
        "bbox = ''\n",
        "# Initialize a counter\n",
        "count = 0\n",
        "# Initialize a list to store the last 7 emotions\n",
        "last_emotions = []\n",
        "# Initialize top emotions\n",
        "top_emotions = []\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
        "\n",
        "    # Grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray)  # Ensure face_cascade is defined\n",
        "\n",
        "    # Get face bounding box for overlay\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract face ROI\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "\n",
        "        # Preprocess the face ROI for the emotion model\n",
        "        resized_face = cv2.resize(face_roi, (80, 80))  # Resize to match model input\n",
        "        normalized_face = resized_face / 255.0  # Normalize pixel values\n",
        "        input_image = np.expand_dims(normalized_face, axis=0)  # Add batch dimension\n",
        "        input_image = np.expand_dims(input_image, axis=-1)  # Add channel dimension for grayscale\n",
        "\n",
        "        # Convert coordinates to integers before drawing the rectangle\n",
        "        x1 = int(.7 * x)\n",
        "        y1 = int(.7 * y)\n",
        "        x2 = int(x + (1.3 * w))\n",
        "        y2 = int(y + (1.3 * h))\n",
        "        bbox_array = cv2.rectangle(bbox_array, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(bbox_array, '', (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
        "\n",
        "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255\n",
        "    # Convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # Update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    # Check if 5 seconds have passed\n",
        "    current_time = time.time()\n",
        "    if current_time - last_snapshot_time >= 5:\n",
        "        count += 1\n",
        "        last_snapshot_time = current_time\n",
        "        snapshot_path = os.path.join(snapshot_dir, f'snapshot_{count}.jpg')\n",
        "        cv2.imwrite(snapshot_path, img)\n",
        "        last_snapshots.append(snapshot_path)\n",
        "        if len(last_snapshots) > 7:\n",
        "            os.remove(last_snapshots.pop(0))\n",
        "\n",
        "        # Perform emotion prediction\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Extract face ROI\n",
        "            face_roi = gray[y:y + h, x:x + w]\n",
        "\n",
        "            # Preprocess the face ROI for the emotion model\n",
        "            resized_face = cv2.resize(face_roi, (80, 80))  # Resize to match model input\n",
        "            # normalized_face = resized_face / 255.0  # Normalize pixel values\n",
        "            input_image = np.expand_dims(resized_face, axis=0)  # Add batch dimension\n",
        "            input_image = np.expand_dims(input_image, axis=-1)  # Add channel dimension for grayscale\n",
        "\n",
        "            # Perform emotion prediction\n",
        "            prediction = emotion_model_html.predict(input_image)[0]\n",
        "\n",
        "            # Get the top three predictions with confidence scores\n",
        "            top_indices = prediction.argsort()[-3:][::-1]\n",
        "            top_emotions = [(emotion_labels[i], prediction[i]) for i in top_indices]\n",
        "\n",
        "            # Store emotion data\n",
        "            emotion_data.append({'timestamp': time.time(), 'emotions': top_emotions})\n",
        "\n",
        "            # Add the top prediction to the last_emotions list\n",
        "            last_emotions.append(top_emotions[0][0])\n",
        "            if len(last_emotions) > 7:\n",
        "                last_emotions.pop(0)\n",
        "\n",
        "        # Clear previous print statements\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # Print the last 7 emotions\n",
        "        print(last_emotions)\n",
        "\n",
        "        # Create a DataFrame from the emotion data\n",
        "        emotion_df = pd.DataFrame(emotion_data)\n",
        "        emotion_df.to_csv('emotion_data.csv', index=False)  # Save the DataFrame to a CSV file\n",
        "\n",
        "        # Print the top three emotions with confidence scores for each face\n",
        "        for top_emotion in top_emotions:\n",
        "            print(f\"{top_emotion[0]}: {top_emotion[1]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' This Block takes images from the live video and runs inference on them with the html model file\n",
        "Then feeds that to our music key to generate a recommendation for the user'''\n",
        "\n",
        "import emotion_is_the_key as eitk\n",
        "import more_than_a_feeling as mtaf\n",
        "\n",
        "# Start streaming video from webcam\n",
        "video_stream()\n",
        "# Label for video\n",
        "label_html = 'Capturing...'\n",
        "# Initialize bounding box to empty\n",
        "bbox = ''\n",
        "# Initialize a counter\n",
        "count = 0\n",
        "# Initialize a list to store the last 7 snapshots\n",
        "last_snapshots = []\n",
        "# Initialize a list to store the last 7 emotions\n",
        "last_emotions = []\n",
        "# Initialize a variable to store the last snapshot time\n",
        "last_snapshot_time = time.time()\n",
        "# Initialize the emotion list\n",
        "emotion_data = []\n",
        "# Initialize top emotions\n",
        "top_emotions = []\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
        "\n",
        "    # Grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray)  # Ensure face_cascade is defined\n",
        "\n",
        "    # Get face bounding box for overlay\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract face ROI\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "\n",
        "        # Preprocess the face ROI for the emotion model\n",
        "        resized_face = cv2.resize(face_roi, (80, 80))  # Resize to match model input\n",
        "        normalized_face = resized_face / 255.0  # Normalize pixel values\n",
        "        input_image = np.expand_dims(normalized_face, axis=0)  # Add batch dimension\n",
        "        input_image = np.expand_dims(input_image, axis=-1)  # Add channel dimension for grayscale\n",
        "\n",
        "        # Convert coordinates to integers before drawing the rectangle\n",
        "        x1 = int(.7 * x)\n",
        "        y1 = int(.7 * y)\n",
        "        x2 = int(x + (1.3 * w))\n",
        "        y2 = int(y + (1.3 * h))\n",
        "        bbox_array = cv2.rectangle(bbox_array, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(bbox_array, '', (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
        "\n",
        "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255\n",
        "    # Convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # Update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "    # Check if 5 seconds have passed\n",
        "    current_time = time.time()\n",
        "    if current_time - last_snapshot_time >= 5:\n",
        "        count += 1\n",
        "        last_snapshot_time = current_time\n",
        "        snapshot_path = os.path.join(snapshot_dir, f'snapshot_{count}.jpg')\n",
        "        cv2.imwrite(snapshot_path, img)\n",
        "        last_snapshots.append(snapshot_path)\n",
        "        if len(last_snapshots) > 7:\n",
        "            os.remove(last_snapshots.pop(0))\n",
        "\n",
        "        # Perform emotion prediction\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Extract face ROI\n",
        "            face_roi = gray[y:y + h, x:x + w]\n",
        "\n",
        "            # Preprocess the face ROI for the emotion model\n",
        "            resized_face = cv2.resize(face_roi, (80, 80))  # Resize to match model input\n",
        "            # normalized_face = resized_face / 255.0  # Normalize pixel values\n",
        "            input_image = np.expand_dims(resized_face, axis=0)  # Add batch dimension\n",
        "            input_image = np.expand_dims(input_image, axis=-1)  # Add channel dimension for grayscale\n",
        "\n",
        "            # Perform emotion prediction\n",
        "            prediction = emotion_model_html.predict(input_image)[0]\n",
        "\n",
        "            # Get the top three predictions with confidence scores\n",
        "            top_indices = prediction.argsort()[-3:][::-1]\n",
        "            top_emotions = [(emotion_labels[i], prediction[i]) for i in top_indices]\n",
        "\n",
        "            # Store emotion data\n",
        "            emotion_data.append({'timestamp': time.time(), 'emotions': top_emotions})\n",
        "\n",
        "            # Add the top prediction to the last_emotions list\n",
        "            last_emotions.append(top_emotions[0][0])\n",
        "            if len(last_emotions) > 7:\n",
        "                last_emotions.pop(0)\n",
        "\n",
        "        # Clear previous print statements\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # Print the last 7 emotions\n",
        "        print(last_emotions)\n",
        "\n",
        "        # Create a DataFrame from the emotion data\n",
        "        emotion_df = pd.DataFrame(emotion_data)\n",
        "        emotion_df.to_csv('emotion_data.csv', index=False)  # Save the DataFrame to a CSV file\n",
        "\n",
        "        # Print the top three emotions with confidence scores for each face\n",
        "        for top_emotion in top_emotions:\n",
        "            print(f\"{top_emotion[0]}: {top_emotion[1]:.2f}\")\n",
        "\n",
        "        # Get the top emotion and the average emotion for the last 7 frames\n",
        "        top_emotion = top_emotions[0][0]\n",
        "        avg_emotion = emotion_df['emotions'].apply(lambda x: x[0][0]).value_counts().idxmax()\n",
        "\n",
        "        '''We want to get the key for the top emotion so we can make an immediate recommendation, \n",
        "        and then after that we want to adjust the song once the average emotion changes'''\n",
        "\n",
        "        # Get the key for the top emotion\n",
        "        top_key = eitk.get_song_title(top_emotion)\n",
        "\n",
        "        # Get the key for the average emotion\n",
        "        avg_key = eitk.get_song_title(avg_emotion)\n",
        "\n",
        "        print(avg_emotion)\n",
        "        print(avg_key)\n",
        "\n",
        "        song_name, artist_name, emotion = avg_key\n",
        "\n",
        "        # Feed the avg_key to the song recommendation code\n",
        "        print(mtaf.recommend_song_from_cluster(mtaf.original_df, mtaf.model, song_name, artist_name, mtaf.client_id, mtaf.client_secret))\n",
        "        \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
